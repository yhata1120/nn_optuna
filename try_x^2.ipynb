{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough GPU hardware devices available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for k in range(len(physical_devices)):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[k], True)\n",
    "        print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[k]))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Dropout, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective関数を内包する高階関数。objective関数呼び出し前に、種々の事前設定等を行う。\n",
    "def outer_objective():\n",
    "    \n",
    "    # ハイパーパラメータの調整設定読み込み\n",
    "    \n",
    "    # batch_size\n",
    "    n_bs_range = (\"log2_batch_size\", 5, 10)\n",
    "    # エポック数\n",
    "    nb_epochs = 10000 \n",
    "    # 収束判定ループ（エポック）回数\n",
    "    nb_patience = 500\n",
    "    # 収束判定用差分パラメータ\n",
    "    val_min_delta = 1e-5\n",
    "    \n",
    "    # 試行する中間層数の範囲設定\n",
    "    n_layer_range = ('n_layer',1,5)\n",
    "\n",
    "    # 試行する中間層のユニット（ニューロン）数の範囲設定\n",
    "    mid_units_range = (\"log2_n_node\", 3, 8)\n",
    "\n",
    "    # 試行するドロップアウト率の範囲設定\n",
    "    dropout_rate_range = ('dropout_rate',0.0,0.5)\n",
    "\n",
    "    # 試行する活性化関数のリスト設定\n",
    "    activation_list = ('activation',['relu','tanh','sigmoid'])\n",
    "\n",
    "    # 試行する最適化アルゴリズムのリスト設定\n",
    "    optimizer_list = ('optimizer',['adam','sgd'])\n",
    "\n",
    "    # 試行する学習率の範囲設定(adam)\n",
    "    lr_adam_range = (\"lr_adam\", 1e-4, 1e-1)\n",
    "    \n",
    "    # 試行する学習率の範囲設定(sgd)\n",
    "    lr_sgd_range = 0.01\n",
    "\n",
    "    # 収束判定設定。以下の条件を満たすエポックがpatience回続いたら打切り。\n",
    "    # val_loss(観測上最小値) - min_delta  < val_loss\n",
    "    es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=val_min_delta, patience=nb_patience, mode='min', restore_best_weights=True)\n",
    "\n",
    "    callbacks = [es_cb]\n",
    "    \n",
    "    print('obj_loop_start')\n",
    "\n",
    "    # 目的関数\n",
    "    def objective(trial):\n",
    "        \n",
    "            # バッチサイズ設定\n",
    "        n_bs = 2**trial.suggest_int(*n_bs_range)\n",
    "\n",
    "        # 中間層数の探索範囲設定\n",
    "        n_layer = trial.suggest_int(*n_layer_range)\n",
    "        # ユニット数の探索範囲設定\n",
    "        mid_units = 2**trial.suggest_int(*mid_units_range)\n",
    "        # ドロップアウト率の探索範囲設定\n",
    "        dropout_rate = trial.suggest_uniform(*dropout_rate_range)\n",
    "        # 活性化関数の探索候補設定\n",
    "        activation = trial.suggest_categorical(*activation_list)\n",
    "        # 最適化アルゴリズムの探索候補設定\n",
    "        optimizer = trial.suggest_categorical(*optimizer_list)\n",
    "        # 最適化アルゴリズムの学習率の探索候補決定(adam)\n",
    "        lr_adam = trial.suggest_loguniform(*lr_adam_range)\n",
    "        # 最適化アルゴリズムの学習率の探索候補決定(sgd)\n",
    "        lr_sgd = lr_sgd_range\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        # 学習モデルの構築と学習の開始\n",
    "        # create_modelに入れるものはインプットとアウトプットの\"次元\"\n",
    "        model_config = create_model(n_features, n_outputs, n_layer, activation, mid_units, dropout_rate, optimizer,get_config = True)\n",
    "        \n",
    "        score = kfold_cv(\n",
    "        model_config,\n",
    "        X_trainval,\n",
    "        y_trainval,\n",
    "        optimizer=optimizer,\n",
    "        lr_adam = lr_adam,\n",
    "        lr_sgd = lr_sgd,\n",
    "        loss='mse',\n",
    "        n_splits=5,\n",
    "        n_bs=n_bs,\n",
    "        nb_epochs = nb_epochs,\n",
    "        save_dir=\"mnist_ffnn_optuna\",\n",
    "        prefix=f\"trial_{trial.number}\",\n",
    "        callbacks = callbacks,\n",
    "        eval_func=None,\n",
    "        random_state=0\n",
    "        )\n",
    "    # k分割交差検証でスコアを出す\n",
    "        return score\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features, n_outputs, n_layer, activation, mid_units, dropout_rate, weights=None, get_config=False):\n",
    "    # ニューラルネットワーク定義\n",
    "    inputs = Input(shape=(n_features,))\n",
    "    x = BatchNormalization()(inputs)\n",
    "    # 中間層数、各ニューロン数、ドロップアウト率の定義\n",
    "    kernel_initializer_dict = {\"relu\": \"he_normal\"}\n",
    "    hidden_layers_list = [Dense(mid_units, activation=activation, kernel_initializer=kernel_initializer_dict.get(activation, \"glorot_normal\"), name=f\"hidden_{i + 1}\") for i in range(n_layer)] #\"activation = relu以外の場合はglorot_normalを使う\"\n",
    "    x = inputs\n",
    "    for layer in hidden_layers_list:\n",
    "        x = layer(x)\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "    # 出力層を定義（ニューロン数は1個）\n",
    "    outputs = Dense(n_outputs, activation='linear')(x)\n",
    "    # 回帰学習モデル作成\n",
    "    model = Model(inputs, outputs) # nn.model\n",
    "    # モデルを返す　\n",
    "    # weightか、configの形か、そのままモデルを帰すのか、今回はconfigを通してfileを返す\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "    if get_config:\n",
    "        return model.get_config()\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(model_config, X_trainval, y_trainval, optimizer=\"adam\",lr_adam = 1e-4,lr_sgd = 0.01, loss=\"mse\", n_splits=5, n_bs=32, nb_epochs=1000, save_dir=\".\", prefix=\"kfold_cv\", callbacks=list(), eval_func=None, random_state=0):\n",
    "    lr_dict = {\"adam\":lr_adam, \"sgd\":lr_sgd}\n",
    "    lr=lr_dict.get(optimizer)\n",
    "    val_scores = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    # 自分の場合はdata_trainvalはndarrayではなくdataframeのまま入れる\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(X_trainval)):\n",
    "        # Prepare dataset\n",
    "        X_train, X_val = X_trainval[train_indices], X_trainval[val_indices]\n",
    "        y_train, y_val = y_trainval[train_indices], y_trainval[val_indices]\n",
    "        # Create model from model_config\n",
    "        if isinstance(model_config, dict):\n",
    "            model = Model.from_config(model_config)\n",
    "            model.compile(optimizer=optimizer, loss=loss)\n",
    "        elif isinstance(model_config, str):\n",
    "            if os.path.isfile(model_config):\n",
    "                with open(model_config, \"rt\") as f:\n",
    "                    json_string = f.read()\n",
    "            else:\n",
    "                json_string = model_config\n",
    "            model = model_from_json(json_string)\n",
    "            model.compile(optimizer=optimizer(lr = lr), loss=loss)\n",
    "        elif callable(model_config):\n",
    "            model = model_config()\n",
    "        else:\n",
    "            raise RuntimeError(f\"unknown type of model_config: {type(model_config)}\")\n",
    "        model._name = f\"{prefix}_{model.name}\"\n",
    "        # Save model architecture\n",
    "        if fold == 0:\n",
    "            with open(os.path.join(save_dir, f\"{prefix}_architecture.json\"), 'wt') as f:\n",
    "                f.write(model.to_json())\n",
    "\n",
    "        # Train model\n",
    "        # validation_dataというoptionを使っている\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        # Save learning history\n",
    "        with open(os.path.join(save_dir, f'{prefix}_cv{fold}_history.pickle'), 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        # Evaluate model by validation data\n",
    "        # もし、何かしらの評価関数(eval_func)がなければmodel.evaluateで追加、\n",
    "        # 何かしらの評価関数(eval_func)が存在すればそちらで誤差を評価\n",
    "        if eval_func is None:\n",
    "            score = model.evaluate(X_val, y_val)\n",
    "        else:\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            score = eval_func(y_val, y_val_pred)\n",
    "        print(f'fold {fold} score: {score}')\n",
    "        # append each fold scores\n",
    "        val_scores.append(score)\n",
    "        # Delete model and clear session # メモリ問題\n",
    "        del model\n",
    "        K.clear_session()\n",
    "    # Get average of validation score\n",
    "    # optunaによる実験一回分のスコア\n",
    "    cv_score = np.mean(val_scores)\n",
    "    print(f'CV score: {cv_score}')\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-27 20:55:11,299]\u001b[0m A new study created in memory with name: no-name-b3b7ffdc-7486-48db-b887-6dd18adaf57e\u001b[0m\n",
      "\u001b[33m[W 2021-09-27 20:55:11,343]\u001b[0m Trial 0 failed because of the following error: NotFoundError()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 96, in NewCheckpointReader\n",
      "    return CheckpointReader(compat.as_bytes(filepattern))\n",
      "RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for adam\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\_optimize.py\", line 216, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-34-dfd2d1664c02>\", line 70, in objective\n",
      "    model_config = create_model(n_features, n_outputs, n_layer, activation, mid_units, dropout_rate, optimizer,get_config = True)\n",
      "  File \"<ipython-input-45-cf476ada4550>\", line 19, in create_model\n",
      "    model.load_weights(weights)\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2329, in load_weights\n",
      "    filepath, save_format = _detect_save_format(filepath)\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 3014, in _detect_save_format\n",
      "    if _is_readable_tf_checkpoint(filepath):\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 3035, in _is_readable_tf_checkpoint\n",
      "    tf.compat.v1.train.NewCheckpointReader(filepath)\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 100, in NewCheckpointReader\n",
      "    error_translator(e)\n",
      "  File \"C:\\Users\\hatay\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 35, in error_translator\n",
      "    raise errors_impl.NotFoundError(None, None, error_message)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for adam\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj_loop_start\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for adam",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m   \u001b[1;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for adam",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-28a9540fb264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# optimizeに最適化すべき目的関数（objective）を渡す。これをn_trials回試行する。目的関数の値が最小のものを探索する。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m# ここではouter_objective()を実行してobjective()を実行している\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    399\u001b[0m             )\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    402\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-dfd2d1664c02>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# 学習モデルの構築と学習の開始\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# create_modelに入れるものはインプットとアウトプットの\"次元\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmid_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         score = kfold_cv(\n",
      "\u001b[1;32m<ipython-input-45-cf476ada4550>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(n_features, n_outputs, n_layer, activation, mid_units, dropout_rate, weights, get_config)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# weightか、configの形か、そのままモデルを帰すのか、今回はconfigを通してfileを返す\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2327\u001b[0m           'True when by_name is True.')\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m     \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m   3012\u001b[0m   \u001b[1;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3013\u001b[0m   \u001b[1;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3014\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3015\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m   3033\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3034\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3035\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3036\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3037\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     98\u001b[0m   \u001b[1;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0merror_translator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     33\u001b[0m       \u001b[1;34m'Failed to find any '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[0;32m     37\u001b[0m       \u001b[1;34m'Data type '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for adam"
     ]
    }
   ],
   "source": [
    "# load data of gb (今回はx^2を使って実験を行う) \n",
    "def dataset():\n",
    "    sample_size = 10000\n",
    "    noise_size = 100\n",
    "\n",
    "    X_all = np.linspace(-100,100,sample_size)\n",
    "    y_all = X_all**2 + noise_size*np.random.randn((len(X_all)))\n",
    "\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(X_all,y_all,test_size=0.2,random_state=0)\n",
    "\n",
    "    return X_trainval, X_test, y_trainval, y_test\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test= dataset()\n",
    "\n",
    "X_trainval = X_trainval.reshape(X_trainval.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "y_trainval = y_trainval.reshape(y_trainval.shape[0], -1)\n",
    "y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "# 検証用のデータと訓練用のデータを準備\n",
    "n_features = len(X_trainval.T)\n",
    "n_outputs = len(y_trainval.T)\n",
    "\n",
    "# 探索試行回数を設定\n",
    "n_trials = 10\n",
    "# 最適化探索（optunaのstudyオブジェクト定義）\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler())\n",
    "# optimizeに最適化すべき目的関数（objective）を渡す。これをn_trials回試行する。目的関数の値が最小のものを探索する。\n",
    "study.optimize(outer_objective(), n_trials)\n",
    "# ここではouter_objective()を実行してobjective()を実行している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
